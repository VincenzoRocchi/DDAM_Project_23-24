{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Malware Detection in Network Traffic Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and spark setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pyspark.sql import SQLContext, SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('Malware').config(\"spark.executor.memory\", \"14g\").getOrCreate()\n",
    "\n",
    "#sc = spark.sparkContext\n",
    "#sqlsc = SQLContext(spark)\n",
    "\n",
    "import os\n",
    "# os.environ['HADOOP_HOME'] = 'C:/dummy/hadoop_home'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('spark.app.submitTime', '1702480620972'), ('spark.sql.warehouse.dir', 'file:/C:/Users/Vincenzo/Projects/DDAM_Project_23-24/code/spark-warehouse'), ('spark.executor.memory', '14g'), ('spark.app.startTime', '1702480621048'), ('spark.app.id', 'local-1702480621676'), ('spark.driver.port', '57701'), ('spark.executor.id', 'driver'), ('spark.driver.extraJavaOptions', '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'), ('spark.app.name', 'Malware'), ('spark.rdd.compress', 'True'), ('spark.driver.host', '192.168.1.143'), ('spark.serializer.objectStreamReset', '100'), ('spark.master', 'local[*]'), ('spark.submit.pyFiles', ''), ('spark.submit.deployMode', 'client'), ('spark.ui.showConsoleProgress', 'true'), ('spark.executor.extraJavaOptions', '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false')]\n"
     ]
    }
   ],
   "source": [
    "print(spark.sparkContext.getConf().getAll())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset concatenation / data assesment / schema corrections [preprocessing]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading files: 100%|██████████| 11/11 [00:00<00:00, 47.06file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------------+---------+---------------+---------+-----+-------+--------+----------+----------+----------+----------+----------+------------+-------+---------+-------------+---------+-------------+--------------+---------+--------------------+\n",
      "|                 ts|      id.orig_h|id.orig_p|      id.resp_h|id.resp_p|proto|service|duration|orig_bytes|resp_bytes|conn_state|local_orig|local_resp|missed_bytes|history|orig_pkts|orig_ip_bytes|resp_pkts|resp_ip_bytes|tunnel_parents|    label|      detailed-label|\n",
      "+-------------------+---------------+---------+---------------+---------+-----+-------+--------+----------+----------+----------+----------+----------+------------+-------+---------+-------------+---------+-------------+--------------+---------+--------------------+\n",
      "|1.525879831015811E9|192.168.100.103|    51524| 65.127.233.163|       23|  tcp|      -|2.999051|       0.0|       0.0|        S0|         -|         -|         0.0|      S|      3.0|        180.0|      0.0|          0.0|             -|Malicious|PartOfAHorizontal...|\n",
      "|1.525879831025055E9|192.168.100.103|    56305|  63.150.16.171|       23|  tcp|      -|    NULL|      NULL|      NULL|        S0|         -|         -|         0.0|      S|      1.0|         60.0|      0.0|          0.0|             -|Malicious|PartOfAHorizontal...|\n",
      "|1.525879831045045E9|192.168.100.103|    41101|   111.40.23.49|       23|  tcp|      -|    NULL|      NULL|      NULL|        S0|         -|         -|         0.0|      S|      1.0|         60.0|      0.0|          0.0|             -|Malicious|PartOfAHorizontal...|\n",
      "| 1.52587983201624E9|192.168.100.103|    60905|131.174.215.147|       23|  tcp|      -|2.998796|       0.0|       0.0|        S0|         -|         -|         0.0|      S|      3.0|        180.0|      0.0|          0.0|             -|Malicious|PartOfAHorizontal...|\n",
      "|1.525879832024985E9|192.168.100.103|    44301|    91.42.47.63|       23|  tcp|      -|    NULL|      NULL|      NULL|        S0|         -|         -|         0.0|      S|      1.0|         60.0|      0.0|          0.0|             -|Malicious|PartOfAHorizontal...|\n",
      "+-------------------+---------------+---------+---------------+---------+-----+-------+--------+----------+----------+----------+----------+----------+------------+-------+---------+-------------+---------+-------------+--------------+---------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- ts: double (nullable = true)\n",
      " |-- id.orig_h: string (nullable = true)\n",
      " |-- id.orig_p: string (nullable = true)\n",
      " |-- id.resp_h: string (nullable = true)\n",
      " |-- id.resp_p: string (nullable = true)\n",
      " |-- proto: string (nullable = true)\n",
      " |-- service: string (nullable = true)\n",
      " |-- duration: double (nullable = true)\n",
      " |-- orig_bytes: double (nullable = true)\n",
      " |-- resp_bytes: double (nullable = true)\n",
      " |-- conn_state: string (nullable = true)\n",
      " |-- local_orig: string (nullable = true)\n",
      " |-- local_resp: string (nullable = true)\n",
      " |-- missed_bytes: double (nullable = true)\n",
      " |-- history: string (nullable = true)\n",
      " |-- orig_pkts: double (nullable = true)\n",
      " |-- orig_ip_bytes: double (nullable = true)\n",
      " |-- resp_pkts: double (nullable = true)\n",
      " |-- resp_ip_bytes: double (nullable = true)\n",
      " |-- tunnel_parents: string (nullable = true)\n",
      " |-- label: string (nullable = true)\n",
      " |-- detailed-label: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "\n",
    "# Define your custom schema\n",
    "custom_schema = StructType([\n",
    "    StructField(\"ts\", DoubleType(), True),\n",
    "    StructField(\"uid\", StringType(), True),\n",
    "    StructField(\"id.orig_h\", StringType(), True),\n",
    "    StructField(\"id.orig_p\", StringType(), True),\n",
    "    StructField(\"id.resp_h\", StringType(), True),\n",
    "    StructField(\"id.resp_p\", StringType(), True),\n",
    "    StructField(\"proto\", StringType(), True),\n",
    "    StructField(\"service\", StringType(), True),\n",
    "    StructField(\"duration\", DoubleType(), True),\n",
    "    StructField(\"orig_bytes\", DoubleType(), True),\n",
    "    StructField(\"resp_bytes\", DoubleType(), True),\n",
    "    StructField(\"conn_state\", StringType(), True),\n",
    "    StructField(\"local_orig\", StringType(), True),\n",
    "    StructField(\"local_resp\", StringType(), True),\n",
    "    StructField(\"missed_bytes\", DoubleType(), True),\n",
    "    StructField(\"history\", StringType(), True),\n",
    "    StructField(\"orig_pkts\", DoubleType(), True),\n",
    "    StructField(\"orig_ip_bytes\", DoubleType(), True),\n",
    "    StructField(\"resp_pkts\", DoubleType(), True),\n",
    "    StructField(\"resp_ip_bytes\", DoubleType(), True),\n",
    "    StructField(\"tunnel_parents\", StringType(), True),\n",
    "    StructField(\"label\", StringType(), True),\n",
    "    StructField(\"detailed-label\", StringType(), True),\n",
    "])\n",
    "\n",
    "# List of file paths\n",
    "file_paths = [\n",
    "    r\"C:\\Users\\Vincenzo\\Projects\\DDAM_data\\malware\\CTU-IoT-Malware-Capture-1-1conn.log.labeled.csv\",\n",
    "    r\"C:\\Users\\Vincenzo\\Projects\\DDAM_data\\malware\\CTU-IoT-Malware-Capture-3-1conn.log.labeled.csv\",\n",
    "    r\"C:\\Users\\Vincenzo\\Projects\\DDAM_data\\malware\\CTU-IoT-Malware-Capture-9-1conn.log.labeled.csv\",\n",
    "    r\"C:\\Users\\Vincenzo\\Projects\\DDAM_data\\malware\\CTU-IoT-Malware-Capture-20-1conn.log.labeled.csv\",\n",
    "    r\"C:\\Users\\Vincenzo\\Projects\\DDAM_data\\malware\\CTU-IoT-Malware-Capture-21-1conn.log.labeled.csv\",\n",
    "    r\"C:\\Users\\Vincenzo\\Projects\\DDAM_data\\malware\\CTU-IoT-Malware-Capture-34-1conn.log.labeled.csv\",\n",
    "    r\"C:\\Users\\Vincenzo\\Projects\\DDAM_data\\malware\\CTU-IoT-Malware-Capture-35-1conn.log.labeled.csv\",\n",
    "    r\"C:\\Users\\Vincenzo\\Projects\\DDAM_data\\malware\\CTU-IoT-Malware-Capture-42-1conn.log.labeled.csv\",\n",
    "    r\"C:\\Users\\Vincenzo\\Projects\\DDAM_data\\malware\\CTU-IoT-Malware-Capture-44-1conn.log.labeled.csv\",\n",
    "    r\"C:\\Users\\Vincenzo\\Projects\\DDAM_data\\malware\\CTU-IoT-Malware-Capture-48-1conn.log.labeled.csv\",\n",
    "    r\"C:\\Users\\Vincenzo\\Projects\\DDAM_data\\malware\\CTU-IoT-Malware-Capture-60-1conn.log.labeled.csv\",\n",
    "]\n",
    "\n",
    "# Initialize an empty DataFrame with the custom schema\n",
    "df = spark.createDataFrame(spark.sparkContext.emptyRDD(), custom_schema)\n",
    "\n",
    "# Use tqdm for progress bar\n",
    "for file_path in tqdm(file_paths, desc=\"Reading files\", unit=\"file\"):\n",
    "    df_temp = spark.read.option(\"escape\", \"\\\"\").option(\"delimiter\", \"|\").csv(file_path, header=True, schema=custom_schema)\n",
    "    df = df.union(df_temp)\n",
    "\n",
    "df = df.drop(\"uid\")\n",
    "\n",
    "df.show(5)\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replacing dots with underscores in column names to avoid errors\n",
    "df = df.toDF(*(c.replace('.', '_') for c in df.columns))\n",
    "df = df.toDF(*(c.replace('-', '_') for c in df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #looking for the coluns that have the '-' value to later work on them\n",
    "# from pyspark.sql.functions import col\n",
    "\n",
    "# columns_with_dash = [col_name for col_name in df.columns if df.filter(col(col_name) == \"-\").count() > 0]\n",
    "# print(\"Columns with '-' values:\", columns_with_dash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "columns_with_dash = ['service', 'local_orig', 'local_resp', 'history', 'tunnel_parents', 'detailed_label']\n",
    "\n",
    "def replace_dash_with_nan(df, columns):\n",
    "\n",
    "    result_df = df\n",
    "    for column in columns:\n",
    "        result_df = result_df.withColumn(column, when(col(column) == '-', None).otherwise(col(column)))\n",
    "\n",
    "    return result_df\n",
    "\n",
    "df = replace_dash_with_nan(df, columns_with_dash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import isnan, when, count, col, isnull\n",
    "# missing = df.select([count(when(isnull(c), c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import from_unixtime, col\n",
    "\n",
    "# df = df.withColumn(\"formatted_ts\", from_unixtime(\"ts\").cast(\"timestamp\"))\n",
    "# df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id_orig_h_features: integer (nullable = true)\n",
      " |-- id_resp_h_features: integer (nullable = true)\n",
      " |-- is_internal_orig: integer (nullable = false)\n",
      " |-- is_internal_resp: integer (nullable = false)\n",
      " |-- common_orig_dest_pairs: long (nullable = false)\n",
      " |-- hour_of_day: integer (nullable = true)\n",
      " |-- day_of_week: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "window_spec = Window.partitionBy('id_orig_h', 'id_resp_h')\n",
    "\n",
    "# Extract features from IP addresses\n",
    "df_features = df.withColumn('id_orig_h_features', F.split('id_orig_h', r'\\.')[0]) \\\n",
    "                .withColumn('id_resp_h_features', F.split('id_resp_h', r'\\.')[0]) \\\n",
    "                .withColumn('is_internal_orig', F.when(F.split('id_orig_h', r'\\.')[0] == '192', 1).otherwise(0)) \\\n",
    "                .withColumn('is_internal_resp', F.when(F.split('id_resp_h', r'\\.')[0] == '192', 1).otherwise(0)) \\\n",
    "                .withColumn('common_orig_dest_pairs', F.count('id_orig_h').over(window_spec)) \\\n",
    "                .withColumn('hour_of_day', F.hour(F.from_unixtime('ts'))) \\\n",
    "                .withColumn('day_of_week', F.dayofweek(F.from_unixtime('ts')))\n",
    "                \n",
    "\n",
    "# Select relevant columns for prediction\n",
    "selected_columns_feature_extraction = ['id_orig_h_features', 'id_resp_h_features', 'is_internal_orig', 'is_internal_resp', 'common_orig_dest_pairs', 'hour_of_day', 'day_of_week']\n",
    "\n",
    "df_selected_IPfeatures = df_features.select(selected_columns_feature_extraction).drop('id_orig_h', 'id_resp_h')\n",
    "\n",
    "# # Convert columns to integer\n",
    "df_selected_IPfeatures = df_selected_IPfeatures.withColumn(\"id_orig_h_features\", col(\"id_orig_h_features\").cast(\"integer\"))\n",
    "df_selected_IPfeatures = df_selected_IPfeatures.withColumn(\"id_resp_h_features\", col(\"id_resp_h_features\").cast(\"integer\"))\n",
    "\n",
    "df_features = df_features.withColumn(\"id_orig_h_features\", col(\"id_orig_h_features\").cast(\"integer\"))\n",
    "df_features = df_features.withColumn(\"id_resp_h_features\", col(\"id_resp_h_features\").cast(\"integer\"))\n",
    "\n",
    "#print schema\n",
    "df_selected_IPfeatures.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id_orig_p_index: double (nullable = false)\n",
      " |-- id_resp_p_index: double (nullable = false)\n",
      " |-- proto_index: double (nullable = false)\n",
      " |-- conn_state_index: double (nullable = false)\n",
      " |-- label_index: double (nullable = false)\n",
      " |-- detailed_label_index: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Columns for StringIndexing\n",
    "\n",
    "string_indexing_cols = [ 'id_orig_p', 'id_resp_p', 'proto', 'conn_state', 'label', 'detailed_label']\n",
    "indexers = [StringIndexer(inputCol=col_name, outputCol=col_name+'_index',handleInvalid='keep') for col_name in string_indexing_cols]\n",
    "\n",
    "# Create a pipeline with StringIndexer stages\n",
    "pipeline = Pipeline(stages=indexers)\n",
    "df_features = pipeline.fit(df_features).transform(df_features)\n",
    "\n",
    "# Select relevant columns for prediction\n",
    "selected_columns_string_index = ['id_orig_p', 'id_resp_p', 'proto', 'conn_state', 'label', 'detailed_label']\n",
    "selected_columns_string_index += [f'{col_name}_index' for col_name in string_indexing_cols]\n",
    "\n",
    "df_selected_string_index = df_features.select(selected_columns_string_index).drop(*string_indexing_cols)\n",
    "\n",
    "#print schema\n",
    "df_selected_string_index.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id_orig_h_features: integer (nullable = true)\n",
      " |-- id_resp_h_features: integer (nullable = true)\n",
      " |-- is_internal_orig: integer (nullable = false)\n",
      " |-- is_internal_resp: integer (nullable = false)\n",
      " |-- common_orig_dest_pairs: long (nullable = false)\n",
      " |-- hour_of_day: integer (nullable = true)\n",
      " |-- day_of_week: integer (nullable = true)\n",
      " |-- id_orig_p_index: double (nullable = false)\n",
      " |-- id_resp_p_index: double (nullable = false)\n",
      " |-- proto_index: double (nullable = false)\n",
      " |-- conn_state_index: double (nullable = false)\n",
      " |-- label_index: double (nullable = false)\n",
      " |-- detailed_label_index: double (nullable = false)\n",
      " |-- ts: double (nullable = true)\n",
      " |-- duration: double (nullable = true)\n",
      " |-- orig_bytes: double (nullable = true)\n",
      " |-- resp_bytes: double (nullable = true)\n",
      " |-- missed_bytes: double (nullable = true)\n",
      " |-- orig_pkts: double (nullable = true)\n",
      " |-- orig_ip_bytes: double (nullable = true)\n",
      " |-- resp_pkts: double (nullable = true)\n",
      " |-- resp_ip_bytes: double (nullable = true)\n",
      " |-- id_orig_h_features: integer (nullable = true)\n",
      " |-- id_resp_h_features: integer (nullable = true)\n",
      " |-- is_internal_orig: integer (nullable = false)\n",
      " |-- is_internal_resp: integer (nullable = false)\n",
      " |-- common_orig_dest_pairs: long (nullable = false)\n",
      " |-- hour_of_day: integer (nullable = true)\n",
      " |-- day_of_week: integer (nullable = true)\n",
      " |-- id_orig_p_index: double (nullable = false)\n",
      " |-- id_resp_p_index: double (nullable = false)\n",
      " |-- proto_index: double (nullable = false)\n",
      " |-- conn_state_index: double (nullable = false)\n",
      " |-- label_index: double (nullable = false)\n",
      " |-- detailed_label_index: double (nullable = false)\n",
      " |-- history: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ip = df_selected_IPfeatures.schema.names\n",
    "string_index = df_selected_string_index.schema.names\n",
    "num_cols = [item[0] for item in df_features.dtypes if item[1] != 'string']\n",
    "\n",
    "selected_features = ip + string_index + num_cols + ['history']\n",
    "\n",
    "df_selected_features = df_features.select(selected_features)\n",
    "df_selected_features.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # dropping the useless columns to start performing the analysis\n",
    "\n",
    "# to_drop_binary = ['ts', 'service', 'duration', 'orig_bytes', 'resp_bytes','local_orig', 'local_resp', 'tunnel_parents', 'detailed_label']\n",
    "# to_drop_multiclass = ['ts', 'service', 'local_orig', 'local_resp', 'tunnel_parents']\n",
    "\n",
    "# df_binary = df_features.drop(*to_drop_binary)\n",
    "# df_multiclass = df_features.drop(*to_drop_multiclass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+----------------+----------------+----------------------+-----------+-----------+---------------+---------------+-----------+----------------+-----------+--------------------+---+--------+----------+----------+------------+---------+-------------+---------+-------------+------------------+------------------+----------------+----------------+----------------------+-----------+-----------+---------------+---------------+-----------+----------------+-----------+--------------------+-------+\n",
      "|id_orig_h_features|id_resp_h_features|is_internal_orig|is_internal_resp|common_orig_dest_pairs|hour_of_day|day_of_week|id_orig_p_index|id_resp_p_index|proto_index|conn_state_index|label_index|detailed_label_index| ts|duration|orig_bytes|resp_bytes|missed_bytes|orig_pkts|orig_ip_bytes|resp_pkts|resp_ip_bytes|id_orig_h_features|id_resp_h_features|is_internal_orig|is_internal_resp|common_orig_dest_pairs|hour_of_day|day_of_week|id_orig_p_index|id_resp_p_index|proto_index|conn_state_index|label_index|detailed_label_index|history|\n",
      "+------------------+------------------+----------------+----------------+----------------------+-----------+-----------+---------------+---------------+-----------+----------------+-----------+--------------------+---+--------+----------+----------+------------+---------+-------------+---------+-------------+------------------+------------------+----------------+----------------+----------------------+-----------+-----------+---------------+---------------+-----------+----------------+-----------+--------------------+-------+\n",
      "|                 0|                 0|               0|               0|                     0|          0|          0|              0|              0|          0|               0|          0|                   0|  0|15265888|  15265888|  15265888|           0|        0|            0|        0|            0|                 0|                 0|               0|               0|                     0|          0|          0|              0|              0|          0|               0|          0|                   0|  25116|\n",
      "+------------------+------------------+----------------+----------------+----------------------+-----------+-----------+---------------+---------------+-----------+----------------+-----------+--------------------+---+--------+----------+----------+------------+---------+-------------+---------+-------------+------------------+------------------+----------------+----------------+----------------------+-----------+-----------+---------------+---------------+-----------+----------------+-----------+--------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import isnan, when, count, col, isnull\n",
    "missing = df_selected_features.select([count(when(isnull(c), c)).alias(c) for c in df_selected_features.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = ['duration', 'orig_bytes', 'resp_bytes']\n",
    "df_selected_features = df_selected_features.drop(*to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o2283.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 64.0 failed 1 times, most recent failure: Lost task 3.0 in stage 64.0 (TID 1516) (192.168.1.143 executor driver): java.lang.OutOfMemoryError: Java heap space\r\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:50)\r\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\r\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.getSortedIterator(UnsafeExternalSorter.java:555)\r\n\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:172)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage13.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.fetchNextRow(WindowExec.scala:118)\r\n\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.<init>(WindowExec.scala:127)\r\n\tat org.apache.spark.sql.execution.window.WindowExec.$anonfun$doExecute$3(WindowExec.scala:107)\r\n\tat org.apache.spark.sql.execution.window.WindowExec$$Lambda$4731/20633237.apply(Unknown Source)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\r\n\tat org.apache.spark.rdd.RDD$$Lambda$3275/24342096.apply(Unknown Source)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.sql.execution.SQLExecutionRDD.$anonfun$compute$1(SQLExecutionRDD.scala:52)\r\n\tat org.apache.spark.sql.execution.SQLExecutionRDD$$Lambda$4756/13573322.apply(Unknown Source)\r\n\tat org.apache.spark.sql.internal.SQLConf$.withExistingConf(SQLConf.scala:158)\r\n\tat org.apache.spark.sql.execution.SQLExecutionRDD.compute(SQLExecutionRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2493)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$1(RDD.scala:1225)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:407)\r\n\tat org.apache.spark.rdd.RDD.aggregate(RDD.scala:1218)\r\n\tat org.apache.spark.ml.tree.impl.DecisionTreeMetadata$.buildMetadata(DecisionTreeMetadata.scala:125)\r\n\tat org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:274)\r\n\tat org.apache.spark.ml.classification.RandomForestClassifier.$anonfun$train$1(RandomForestClassifier.scala:168)\r\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\r\n\tat scala.util.Try$.apply(Try.scala:213)\r\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\r\n\tat org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:139)\r\n\tat org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:47)\r\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:114)\r\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:78)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.lang.OutOfMemoryError: Java heap space\r\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:50)\r\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\r\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.getSortedIterator(UnsafeExternalSorter.java:555)\r\n\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:172)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage13.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.fetchNextRow(WindowExec.scala:118)\r\n\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.<init>(WindowExec.scala:127)\r\n\tat org.apache.spark.sql.execution.window.WindowExec.$anonfun$doExecute$3(WindowExec.scala:107)\r\n\tat org.apache.spark.sql.execution.window.WindowExec$$Lambda$4731/20633237.apply(Unknown Source)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\r\n\tat org.apache.spark.rdd.RDD$$Lambda$3275/24342096.apply(Unknown Source)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.sql.execution.SQLExecutionRDD.$anonfun$compute$1(SQLExecutionRDD.scala:52)\r\n\tat org.apache.spark.sql.execution.SQLExecutionRDD$$Lambda$4756/13573322.apply(Unknown Source)\r\n\tat org.apache.spark.sql.internal.SQLConf$.withExistingConf(SQLConf.scala:158)\r\n\tat org.apache.spark.sql.execution.SQLExecutionRDD.compute(SQLExecutionRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 22\u001b[0m\n\u001b[0;32m     20\u001b[0m rf_classifier \u001b[38;5;241m=\u001b[39m RandomForestClassifier(featuresCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m'\u001b[39m, labelCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhistory_index\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     21\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m Pipeline(stages\u001b[38;5;241m=\u001b[39m[rf_classifier])\n\u001b[1;32m---> 22\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Step 5: Use the trained model to predict missing values\u001b[39;00m\n\u001b[0;32m     25\u001b[0m df_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtransform(df_selected_features)\n",
      "File \u001b[1;32mc:\\Users\\Vincenzo\\Projects\\DDAM_Project_23-24\\venv\\Lib\\site-packages\\pyspark\\ml\\base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[0;32m    210\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Vincenzo\\Projects\\DDAM_Project_23-24\\venv\\Lib\\site-packages\\pyspark\\ml\\pipeline.py:134\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    132\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m stage\u001b[38;5;241m.\u001b[39mtransform(dataset)\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# must be an Estimator\u001b[39;00m\n\u001b[1;32m--> 134\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mstage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    135\u001b[0m     transformers\u001b[38;5;241m.\u001b[39mappend(model)\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m indexOfLastEstimator:\n",
      "File \u001b[1;32mc:\\Users\\Vincenzo\\Projects\\DDAM_Project_23-24\\venv\\Lib\\site-packages\\pyspark\\ml\\base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[0;32m    210\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Vincenzo\\Projects\\DDAM_Project_23-24\\venv\\Lib\\site-packages\\pyspark\\ml\\wrapper.py:381\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[1;32m--> 381\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    382\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n\u001b[0;32m    383\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n",
      "File \u001b[1;32mc:\\Users\\Vincenzo\\Projects\\DDAM_Project_23-24\\venv\\Lib\\site-packages\\pyspark\\ml\\wrapper.py:378\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    377\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[1;32m--> 378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Vincenzo\\Projects\\DDAM_Project_23-24\\venv\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\Vincenzo\\Projects\\DDAM_Project_23-24\\venv\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\Users\\Vincenzo\\Projects\\DDAM_Project_23-24\\venv\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o2283.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 64.0 failed 1 times, most recent failure: Lost task 3.0 in stage 64.0 (TID 1516) (192.168.1.143 executor driver): java.lang.OutOfMemoryError: Java heap space\r\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:50)\r\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\r\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.getSortedIterator(UnsafeExternalSorter.java:555)\r\n\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:172)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage13.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.fetchNextRow(WindowExec.scala:118)\r\n\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.<init>(WindowExec.scala:127)\r\n\tat org.apache.spark.sql.execution.window.WindowExec.$anonfun$doExecute$3(WindowExec.scala:107)\r\n\tat org.apache.spark.sql.execution.window.WindowExec$$Lambda$4731/20633237.apply(Unknown Source)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\r\n\tat org.apache.spark.rdd.RDD$$Lambda$3275/24342096.apply(Unknown Source)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.sql.execution.SQLExecutionRDD.$anonfun$compute$1(SQLExecutionRDD.scala:52)\r\n\tat org.apache.spark.sql.execution.SQLExecutionRDD$$Lambda$4756/13573322.apply(Unknown Source)\r\n\tat org.apache.spark.sql.internal.SQLConf$.withExistingConf(SQLConf.scala:158)\r\n\tat org.apache.spark.sql.execution.SQLExecutionRDD.compute(SQLExecutionRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2493)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$aggregate$1(RDD.scala:1225)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:407)\r\n\tat org.apache.spark.rdd.RDD.aggregate(RDD.scala:1218)\r\n\tat org.apache.spark.ml.tree.impl.DecisionTreeMetadata$.buildMetadata(DecisionTreeMetadata.scala:125)\r\n\tat org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:274)\r\n\tat org.apache.spark.ml.classification.RandomForestClassifier.$anonfun$train$1(RandomForestClassifier.scala:168)\r\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\r\n\tat scala.util.Try$.apply(Try.scala:213)\r\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\r\n\tat org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:139)\r\n\tat org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:47)\r\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:114)\r\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:78)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.lang.OutOfMemoryError: Java heap space\r\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:50)\r\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\r\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.getSortedIterator(UnsafeExternalSorter.java:555)\r\n\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:172)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage13.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.fetchNextRow(WindowExec.scala:118)\r\n\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.<init>(WindowExec.scala:127)\r\n\tat org.apache.spark.sql.execution.window.WindowExec.$anonfun$doExecute$3(WindowExec.scala:107)\r\n\tat org.apache.spark.sql.execution.window.WindowExec$$Lambda$4731/20633237.apply(Unknown Source)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\r\n\tat org.apache.spark.rdd.RDD$$Lambda$3275/24342096.apply(Unknown Source)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.sql.execution.SQLExecutionRDD.$anonfun$compute$1(SQLExecutionRDD.scala:52)\r\n\tat org.apache.spark.sql.execution.SQLExecutionRDD$$Lambda$4756/13573322.apply(Unknown Source)\r\n\tat org.apache.spark.sql.internal.SQLConf$.withExistingConf(SQLConf.scala:158)\r\n\tat org.apache.spark.sql.execution.SQLExecutionRDD.compute(SQLExecutionRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Vincenzo\\Projects\\DDAM_Project_23-24\\venv\\Lib\\site-packages\\py4j\\clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Vincenzo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\socket.py\", line 707, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Vincenzo\\Projects\\DDAM_Project_23-24\\venv\\Lib\\site-packages\\py4j\\java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Vincenzo\\Projects\\DDAM_Project_23-24\\venv\\Lib\\site-packages\\py4j\\clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 57751)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Vincenzo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\socketserver.py\", line 318, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"C:\\Users\\Vincenzo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\socketserver.py\", line 349, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"C:\\Users\\Vincenzo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\socketserver.py\", line 362, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"C:\\Users\\Vincenzo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\socketserver.py\", line 761, in __init__\n",
      "    self.handle()\n",
      "  File \"c:\\Users\\Vincenzo\\Projects\\DDAM_Project_23-24\\venv\\Lib\\site-packages\\pyspark\\accumulators.py\", line 295, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"c:\\Users\\Vincenzo\\Projects\\DDAM_Project_23-24\\venv\\Lib\\site-packages\\pyspark\\accumulators.py\", line 267, in poll\n",
      "    if self.rfile in r and func():\n",
      "                           ^^^^^^\n",
      "  File \"c:\\Users\\Vincenzo\\Projects\\DDAM_Project_23-24\\venv\\Lib\\site-packages\\pyspark\\accumulators.py\", line 271, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Vincenzo\\Projects\\DDAM_Project_23-24\\venv\\Lib\\site-packages\\pyspark\\serializers.py\", line 594, in read_int\n",
      "    length = stream.read(4)\n",
      "             ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Vincenzo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\socket.py\", line 707, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Step 1: Drop rows with missing labels\n",
    "df = df_selected_features.filter(F.col('history').isNotNull())\n",
    "\n",
    "# Step 2: Convert \"history\" to numerical indices\n",
    "history_indexer = StringIndexer(inputCol='history', outputCol='history_index')\n",
    "df = history_indexer.fit(df).transform(df)\n",
    "\n",
    "# Step 3: Prepare features and labels\n",
    "features = df.schema.names\n",
    "features.remove('history')\n",
    "vector_assembler = VectorAssembler(inputCols=features, outputCol='features')\n",
    "df = vector_assembler.transform(df)\n",
    "\n",
    "# Step 4: Train RandomForest model\n",
    "rf_classifier = RandomForestClassifier(featuresCol='features', labelCol='history_index')\n",
    "pipeline = Pipeline(stages=[rf_classifier])\n",
    "model = pipeline.fit(df)\n",
    "\n",
    "# Step 5: Use the trained model to predict missing values\n",
    "df_pred = model.transform(df_selected_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import col\n",
    "\n",
    "# # Assume 'prediction' is the column containing the predicted values\n",
    "# # and 'history' is the original column with missing values\n",
    "\n",
    "# # Select only the relevant columns for substitution\n",
    "# df_substitute = df_pred.select('history_index', 'prediction')\n",
    "\n",
    "# # Replace the missing values in the 'history' column with the predicted values\n",
    "# df_result = df_binary.join(df_substitute, on='history_index', how='left_outer') \\\n",
    "#     .withColumn('history', F.when(col('history').isNotNull(), col('history')).otherwise(col('prediction'))) \\\n",
    "#     .drop('history_index', 'prediction')\n",
    "\n",
    "# # Show the resulting DataFrame\n",
    "# df_result.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dimension and content analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rows, cols = df.count(), len(df.columns)\n",
    "# print(f'Dimension of the whole Dataframe is: {(rows,cols)}')\n",
    "\n",
    "# rows, cols = df_binary.count(), len(df_binary.columns)\n",
    "# print(f'Dimension of the binary Dataframe is: {(rows,cols)}')\n",
    "\n",
    "# rows, cols = df_multiclass.count(), len(df_multiclass.columns)\n",
    "# print(f'Dimension of the multiclass Dataframe is: {(rows,cols)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_cols = [item[0] for item in df.dtypes if item[1] != 'string']\n",
    "# print('Le colonne numeriche sono {}'.format(len(num_cols)))\n",
    "# print(num_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# non_num_cols = [item[0] for item in df.dtypes if item[1] == 'string']\n",
    "# print('Le colonne non numeriche sono {}'.format(len(non_num_cols)))\n",
    "# print(non_num_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for missing labels for each file to understandand labelling consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import isnan, when, count, col, isnull\n",
    "\n",
    "\n",
    "# def check_nan(spark, column_to_check=\"\", label_only=False):\n",
    "#     # List of file numbers\n",
    "#     file_numbers = [1, 3, 9, 20, 21, 34, 35, 42, 44, 48, 60]\n",
    "    \n",
    "#     for number in tqdm(file_numbers, desc=\"Processing files\", unit=\"file\"):\n",
    "    \n",
    "#         file_path = r\"C:\\Users\\Vincenzo\\Projects\\DDAM_data\\malware\\CTU-IoT-Malware-Capture-{}-1conn.log.labeled.csv\".format(number)\n",
    "#         df = spark.read.option(\"escape\", \"\\\"\").option(\"delimiter\", \"|\").csv(file_path, header='true', inferSchema='true')\n",
    "#         df = df.toDF(*(c.replace('.', '_') for c in df.columns))\n",
    "\n",
    "#         print(\"\", flush=True)\n",
    "#         # Check for missing values\n",
    "#         if label_only:\n",
    "#             missing = df.select([count(when(isnull(\"detailed-label\"), \"detailed-label\")).alias(\"missing_count\")])\n",
    "#             print(\"Missing values in file {}: \".format(number))\n",
    "#             missing.select(\"missing_count\").show()\n",
    "\n",
    "#         elif column_to_check:\n",
    "#             missing = df.select([count(when(isnull(column_to_check), column_to_check)).alias(\"missing_count\")])\n",
    "#             print(\"Missing values in file {}: \".format(number))\n",
    "#             missing.select(\"missing_count\").show()\n",
    "\n",
    "#         elif column_to_check == \"all\":\n",
    "#             missing = df.select([count(when(isnull(c), c)).alias(c) for c in df.columns])\n",
    "#             print(\"Missing values in file {}: \".format(number))\n",
    "#             missing.show()\n",
    "\n",
    "#         else:\n",
    "#             print(\"Please enter a valid column name or enter True to check the label column only\")\n",
    "\n",
    "# # Check for missing values in the label column only\n",
    "# check_nan(spark, label_only=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for distinct values and raw statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import col, countDistinct\n",
    "\n",
    "# non_num_df = df.select(non_num_cols)\n",
    "\n",
    "# def count_distinct_values(df):\n",
    "#     result = {}\n",
    "#     columns = df.columns\n",
    "\n",
    "#     # Use tqdm to create a progress bar\n",
    "#     for column in tqdm(columns, desc=\"Counting Distinct Values\", unit=\"column\"):\n",
    "#         distinct_count = df.select(column).agg(countDistinct(column)).collect()[0][0]\n",
    "#         result[column] = distinct_count\n",
    "\n",
    "#     return result\n",
    "\n",
    "# distinct_counts = count_distinct_values(non_num_df)\n",
    "\n",
    "# for column, count in distinct_counts.items():\n",
    "#     print(f\"Column '{column}' has {count} distinct values.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Use the describe function to get statistical summary\n",
    "\n",
    "# summary = df.describe()\n",
    "# summary.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # num_summary = summary.select(*num_cols)\n",
    "# num_summary = summary.select(*([\"summary\"] + num_cols))\n",
    "# # non_num_summary = summary.select(*non_num_cols)\n",
    "# non_num_summary = summary.select(*([\"summary\"] + non_num_cols))\n",
    "\n",
    "# print(\"Summary of numeric columns:\")\n",
    "# num_summary.show()\n",
    "\n",
    "# print(\"Summary of non-numeric columns:\")\n",
    "# non_num_summary.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
